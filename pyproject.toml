[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "llm-server"
version = "0.1.0"
description = "OpenAI-compatible HTTP wrapper for the llm library"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
dependencies = [
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
    "llm>=0.19.0",
    "llm-vertex",
    "llm-gemini",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "httpx>=0.25.0",
]
vscode = [
    "json5>=0.9.0",  # For parsing VS Code JSONC settings
]

[project.scripts]
llm-server = "llm_server.main:run"
configure-vscode = "configure_vscode:main"

[tool.setuptools.packages.find]
where = ["."]
include = ["llm_server*"]

[tool.setuptools]
py-modules = ["configure_vscode"]

[tool.uv.sources]
llm-vertex = { git = "https://github.com/c0ffee0wl/llm-vertex" }
llm-gemini = { git = "https://github.com/c0ffee0wl/llm-gemini" }
